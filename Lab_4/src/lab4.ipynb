{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Laboratorio 4: Detecci贸n de Movimiento y Seguimiento de Objetos** 锔硷\n",
    "\n",
    "**Asignatura:** Visi贸n por Ordenador I  \n",
    "**Grado:** Ingenier铆a Matem谩tica e Inteligencia Artificial  \n",
    "**Curso:** 2024/2025  \n",
    "\n",
    "### **Objetivo**\n",
    "En esta pr谩ctica, aprender谩 a implementar algoritmos de detecci贸n de movimiento mediante sustracci贸n de fondo y flujo 贸ptico, y explorar谩 el filtro de Kalman para el seguimiento de objetos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Materiales**\n",
    "- **Python 3.8+**\n",
    "- **OpenCV**: Puede instalarlo con `pip install opencv-python`\n",
    "- **Dataset de video**: Se usar谩 un archivo de v铆deo o la c谩mara en tiempo real para probar los m茅todos de detecci贸n de movimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Apartado A: Sustracci贸n de Fondo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tarea A.1**: Carga de V铆deo\n",
    "Cargue un v铆deo en el cual se detectar谩n objetos en movimiento. Puede utilizar\n",
    "un v铆deo local o la c谩mara en tiempo real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create a method that reads a video file (using VideoCapture from OpenCV) and returns its frames along with video properties\n",
    "def read_video(videopath):\n",
    "    \"\"\"\n",
    "    Reads a video file and returns its frames along with video properties.\n",
    "\n",
    "    Args:\n",
    "        videopath (str): The path to the video file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - frames (list): A list of frames read from the video.\n",
    "            - frame_width (int): The width of the video frames.\n",
    "            - frame_height (int): The height of the video frames.\n",
    "            - frame_rate (float): The frame rate of the video.\n",
    "    \"\"\"\n",
    "\n",
    "    #TODO: Complete this line to read the video file\n",
    "    cap = cv2.VideoCapture(videopath) \n",
    "    \n",
    "    #TODO: Check if the video was successfully opened\n",
    "    if not cap.isOpened():\n",
    "        print('Error: Could not open the video file')\n",
    "\n",
    "    #TODO: Get the szie of frames and the frame rate of the video\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    #TODO: Use a loop to read the frames of the video and store them in a list\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return frames, frame_width, frame_height, frame_rate\n",
    "\n",
    "#TODO: Path to the video file (visiontraffic.avi)\n",
    "videopath = '../data/visiontraffic.avi'\n",
    "\n",
    "frames, frame_width, frame_height, frame_rate = read_video(videopath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tarea A.2**: Sustracci贸n de Fondo mediante diferencia de frames\n",
    "Realice una sustracci贸n de fondo mediante diferencia de frames, para ello guarde\n",
    "un frame con el fondo est谩tico y 煤selo como frame de referencia de fondo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 241 selected as reference frame\n"
     ]
    }
   ],
   "source": [
    "#TODO:  Show the frames to select the reference frame, press 'n' to move to the next frame and 's' to select the frame\n",
    "for i, frame in enumerate(frames):\n",
    "    #TODO: Show the frame\n",
    "    cv2.imshow('Video', frame)\n",
    "    # Wait for the key\n",
    "    key = cv2.waitKey(0)\n",
    "    # If the key is 'n' continue to the next frame\n",
    "    if key == ord('n'):\n",
    "        continue\n",
    "    # If the key is 's' select the frame as the reference frame\n",
    "    elif key == ord('s'):\n",
    "        #TODO: Copy the frame to use it as a reference\n",
    "        reference_frame = frame\n",
    "        #TODO: Convert the reference frame to grayscale\n",
    "        reference_frame = cv2.cvtColor(reference_frame, cv2.COLOR_BGR2GRAY)\n",
    "        print('Frame {} selected as reference frame'.format(i))\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "#TODO: Compute the difference between the reference frame and the rest of the frames and show the difference\n",
    "for frame in frames:\n",
    "    # Convert the frame to grayscale\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    #TODO: Compute the difference between the reference frame and the current frame\n",
    "    diff = cv2.absdiff(frame, reference_frame)\n",
    "    cv2.imshow('Diferencia', diff)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tarea A.3**: Configuraci贸n de la sustracci贸n de fondo con GMM\n",
    "Configure el sustractor de fondo usando el modelo de mezcla de gaussianas\n",
    "adaptativas (MOG2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Use MOG2 to detect the moving objects in the video\n",
    "\n",
    "history = 500  # Number of frames to use to build the background model\n",
    "varThreshold =   16 # Threshold to detect the background\n",
    "detectShadows = True  # If True the algorithm detects the shadows\n",
    "\n",
    "#TODO: Create the MOG2 object\n",
    "mog2 = cv2.createBackgroundSubtractorMOG2(history=history, varThreshold=varThreshold, detectShadows=detectShadows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tarea A.4**: Aplicaci贸n de la Sustracci贸n de Fondo\n",
    "\n",
    "Aplique la sustracci贸n de fondo en cada frame para extraer los objetos en movimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed video saved at ../output_videos\\output_500_5_True.avi\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# TODO: Create a folder to store the videos\n",
    "output_folder = \"../output_videos\"\n",
    "folder_path = os.path.join(output_folder)\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "\n",
    "# TODO: Name of the output video file with the parameters (history, varThreshold, detectShadows)\n",
    "videoname = f'output_{history}_{varThreshold}_{detectShadows}.avi'\n",
    "output_path = os.path.join(folder_path, videoname)\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(\"../data/visiontraffic.avi\")\n",
    "if not cap.isOpened():\n",
    "    raise ValueError(\"Unable to open video file\")\n",
    "\n",
    "frame_size = (frame_width, frame_height)\n",
    "\n",
    "# TODO: Create a VideoWriter object to save the video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Codec for AVI format\n",
    "out = cv2.VideoWriter(output_path, fourcc, frame_rate, frame_size)\n",
    "\n",
    "# Process each frame\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # TODO: Apply the MOG2 algorithm to detect the moving objects\n",
    "    mask = mog2.apply(frame)\n",
    "\n",
    "    # TODO: Convert to BGR the mask to store it in the video\n",
    "    mask_bgr = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # TODO: Save the mask in a video\n",
    "    out.write(mask_bgr)\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "print(f\"Processed video saved at {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preguntas del Apartado A**\n",
    "1. 驴C贸mo afecta la variable `varThreshold` a la precisi贸n de la detecci贸n?\n",
    "2. 驴Qu茅 ventajas presenta `createBackgroundSubtractorMOG2` frente a m茅todos simples de diferencia de im谩genes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Cuanto mayor la variable, mas se saca de la imagen original. Es decir, un threshold bajo no filtrar谩 suficiente la imagen, mientras que un threshold demasiado alto filtrar谩 tambi茅n las partes que queremos conservar en el v谩lido.\n",
    "\n",
    "2. ofrece un modelo adaptativo que es robusto ante variaciones como luces y movimientos repetitivos. A diferencia de la simple diferencia de im谩genes, MOG2 reduce falsos positivos y es m谩s adecuado para escenarios complejos o en tiempo real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Apartado B: Flujo ptico**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tarea B.1**: Configuraci贸n del Flujo ptico\n",
    "\n",
    "Consulte la documentaci贸n de `cv2.calcOpticalFlowPyrLK` para ver qu茅 parametros se deben definir para realizar el c谩lculo del flujo 贸ptico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Use the method to read the video file (slow_traffic_small.avi)\n",
    "videopath = '../data/slow_traffic_small.mp4'  # Path to the video file\n",
    "frames, frame_width, frame_height, frame_rate = read_video(videopath)\n",
    "\n",
    "#TODO: Define the parameters for Lucas-Kanade optical flow\n",
    "winSize=(15, 15)\n",
    "maxLevel=3\n",
    "criteria= (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tarea B.2**: Detecci贸n de Puntos de Inter茅s\n",
    "\n",
    "Detecte los puntos de inter茅s iniciales usando el algoritmo de Shi-Tomasi (`cv2.goodFeaturesToTrack`) en el primer frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Detect the initial points of interest in the first frame\n",
    "\n",
    "#TODO: Convert the first frame to grayscale\n",
    "prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#TODO: Define the parameters of the Shi-Tomasi algorithm\n",
    "mask = None\n",
    "maxCorners = 100\n",
    "qualityLevel = 0.3\n",
    "minDistance = 7\n",
    "blockSize = 7\n",
    "\n",
    "# Use the function goodFeaturesToTrack to detect the points of interest\n",
    "p0 = cv2.goodFeaturesToTrack(prev_gray, mask=mask, maxCorners=maxCorners, qualityLevel=qualityLevel, minDistance=minDistance, blockSize=blockSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tarea B.3**: C谩lculo y Visualizaci贸n del Flujo ptico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Use a loop to track the points of interest in the rest of the frames\n",
    "\n",
    "# Create a mask image for drawing purposes\n",
    "mask = np.zeros_like(frame)\n",
    "\n",
    "for i, frame in enumerate(frames[1:]):\n",
    "    \n",
    "    input_frame = frame.copy()\n",
    "\n",
    "    \n",
    "    frame_gray = cv2.cvtColor(input_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "   \n",
    "    p1, st, err = cv2.calcOpticalFlowPyrLK(prev_gray, frame_gray, p0, None)\n",
    "\n",
    "    \n",
    "    good_new = p1[st == 1]\n",
    "    good_old = p0[st == 1]\n",
    "\n",
    "    \n",
    "    for j, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "        a, b = new.ravel().astype(int)\n",
    "        c, d = old.ravel().astype(int)\n",
    "        input_frame = cv2.circle(input_frame, (a, b), 5, (0, 0, 255), -1)\n",
    "        mask = cv2.line(mask, (a, b), (c, d), (0, 255, 0), 2)\n",
    "\n",
    "   \n",
    "    prev_gray = frame_gray.copy()  \n",
    "    p0 = good_new.reshape(-1, 1, 2)  \n",
    "\n",
    "   \n",
    "    cv2.imshow('Frame', cv2.add(input_frame, mask))\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preguntas del Apartado B**\n",
    "1. 驴Qu茅 efecto tiene el par谩metro `winSize` en la precisi贸n del flujo 贸ptico?\n",
    "2. 驴C贸mo influye el par谩metro `qualityLevel` en la funci贸n `cv2.goodFeaturesToTrack` al detectar puntos de inter茅s?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. define la ventana en la que se fija el algoritmo para el flujo. Cuanto mas peque帽a, mas precisa para moviminetos finos, pero mas sensible al ruido. Cuanto mas grande, mas robusto al ruido, pero puede perder precisi贸n.\n",
    "2. Valores altos seleccionan solo los puntos m谩s fuertes y confiables, mientras que valores bajos permiten detectar m谩s puntos, incluidos algunos menos relevantes o m谩s susceptibles al ruido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Apartado C: Filtro de Kalman para Seguimiento de Objetos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tarea C.1**: Configuraci贸n del Filtro de Kalman\n",
    "\n",
    "Inicialice el filtro de Kalman (`cv2.KalmanFilter`) con una matriz de medici贸n y transici贸n adecuada para un seguimiento en dos dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial position selected: 222, 160\n"
     ]
    }
   ],
   "source": [
    "videopath = '../data/slow_traffic_small.mp4'  \n",
    "frames, frame_width, frame_height, frame_rate = read_video(videopath)\n",
    "\n",
    "\n",
    "kf = cv2.KalmanFilter(4, 2)  \n",
    "\n",
    "\n",
    "kf.measurementMatrix = np.array([[1, 0, 0, 0],\n",
    "                                 [0, 1, 0, 0]], np.float32)  \n",
    "\n",
    "kf.transitionMatrix = np.array([[1, 0, 1, 0],\n",
    "                                [0, 1, 0, 1],\n",
    "                                [0, 0, 1, 0],\n",
    "                                [0, 0, 0, 1]], np.float32)  \n",
    "\n",
    "kf.processNoiseCov = np.eye(4, dtype=np.float32) * 1e-4  \n",
    "\n",
    "measurement = np.zeros((2, 1), np.float32)  \n",
    "prediction = np.zeros((2, 1), np.float32)  \n",
    "\n",
    "\n",
    "for i, frame in enumerate(frames):\n",
    "   \n",
    "    cv2.imshow('Frame', frame)\n",
    "   \n",
    "    key = cv2.waitKey(0)\n",
    "   \n",
    "    if key == ord('n'):\n",
    "        continue\n",
    "    \n",
    "    elif key == ord('s'):\n",
    "      \n",
    "        x, y, w, h = cv2.selectROI('Frame', frame, False)\n",
    "        track_window = (x, y, w, h)\n",
    "\n",
    "        cx = x + w // 2\n",
    "        cy = y + h // 2\n",
    "\n",
    "       \n",
    "        kf.statePost = np.array([[cx], [cy], [0], [0]], np.float32)\n",
    "\n",
    "        # Inicializar la matriz de covarianza\n",
    "        kf.errorCovPost = np.eye(4, dtype=np.float32)\n",
    "\n",
    "        prediction = kf.predict()\n",
    "\n",
    "        measurement = np.array([[cx], [cy]], np.float32)\n",
    "        kf.correct(measurement)\n",
    "\n",
    "        crop = frame[y:y + h, x:x + w].copy()\n",
    "\n",
    "        hsv_crop = cv2.cvtColor(crop, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        crop_hist = cv2.calcHist([hsv_crop], [0], mask=None, histSize=[180], ranges=[0, 180])\n",
    "        cv2.normalize(crop_hist, crop_hist, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "        print(f'Initial position selected: {x}, {y}')\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tarea C.2**: Predicci贸n y Correcci贸n del Estado\n",
    "\n",
    "Realice la predicci贸n del estado y corrija la posici贸n estimada en cada iteraci贸n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_crit = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 30, 1)\n",
    "\n",
    "for frame in frames[i:]:\n",
    "    input_frame = frame.copy()\n",
    "    img_hsv = cv2.cvtColor(input_frame, cv2.COLOR_BGR2HSV)\n",
    "    img_bproject = cv2.calcBackProject([img_hsv], [0], crop_hist, [0, 180], 1)\n",
    "\n",
    "    ret, track_window = cv2.meanShift(img_bproject, track_window, term_crit)\n",
    "    x_, y_, w_, h_ = track_window\n",
    "\n",
    "    c_x = x_ + w_ // 2\n",
    "    c_y = y_ + h_ // 2\n",
    "\n",
    "    prediction = kf.predict()\n",
    "    measurement = np.array([[c_x], [c_y]], np.float32)\n",
    "    kf.correct(measurement)\n",
    "\n",
    "    cv2.circle(input_frame, (int(prediction[0][0]), int(prediction[1][0])), 5, (0, 0, 255), -1)\n",
    "    \n",
    "    cv2.circle(input_frame, (int(c_x), int(c_y)), 5, (0, 255, 0), -1)\n",
    "    cv2.imshow('Frame', input_frame)\n",
    "    \n",
    "\n",
    "    key = cv2.waitKey(1) \n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preguntas del Apartado C**\n",
    "1. 驴C贸mo afecta el valor de `transitionMatrix` a la predicci贸n en el filtro de Kalman?\n",
    "2. 驴Cu谩l es la diferencia entre `measurementMatrix` y `transitionMatrix` en el contexto del seguimiento de objetos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define la relaci贸n entre el estado anterior y el siguiente, describiendo c贸mo se espera que cambien variables como la posici贸n y velocidad.\n",
    "2. La transitionMatrix predice el siguiente estado, mientras que la measurementMatrix ajusta las predicciones basadas en las mediciones observadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ejercicio Adicional: Exploraci贸n del Modelo de Mezcla de Gaussianas (GMM)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objetivo**: Investigue c贸mo funciona el modelo de mezcla de gaussianas (GMM) para mejorar la detecci贸n en condiciones de iluminaci贸n cambiantes.\n",
    "\n",
    "1. Implementaci贸n del GMM: Utilice `cv2.createBackgroundSubtractorMOG()` y ajuste el par谩metro `history` para observar c贸mo cambia la detecci贸n en funci贸n de la duraci贸n de la memoria del fondo.\n",
    "2. Comparaci贸n con `MOG2`: Observe las diferencias en la sensibilidad a las sombras y los cambios de iluminaci贸n. Pruebe con v铆deos que incluyan cambios graduales de iluminaci贸n y objetos que se detienen temporalmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preguntas del Ejercicio Adicional**\n",
    "1. 驴Qu茅 ventajas observa en `createBackgroundSubtractorMOG` en comparaci贸n con `createBackgroundSubtractorMOG2`?\n",
    "2. 驴C贸mo afecta el par谩metro `history` al rendimiento de detecci贸n en escenas con objetos que aparecen y desaparecen?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
