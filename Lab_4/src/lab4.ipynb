{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Laboratorio 4: Detección de Movimiento y Seguimiento de Objetos** ⚙️🖼️\n",
    "\n",
    "**Asignatura:** Visión por Ordenador I  \n",
    "**Grado:** Ingeniería Matemática e Inteligencia Artificial  \n",
    "**Curso:** 2024/2025  \n",
    "\n",
    "### **Objetivo**\n",
    "En esta práctica, aprenderá a implementar algoritmos de detección de movimiento mediante sustracción de fondo y flujo óptico, y explorará el filtro de Kalman para el seguimiento de objetos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Materiales**\n",
    "- **Python 3.8+**\n",
    "- **OpenCV**: Puede instalarlo con `pip install opencv-python`\n",
    "- **Dataset de video**: Se usará un archivo de vídeo o la cámara en tiempo real para probar los métodos de detección de movimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Apartado A: Sustracción de Fondo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tarea A.1**: Carga de Vídeo\n",
    "Cargue un vídeo en el cual se detectarán objetos en movimiento. Puede utilizar\n",
    "un vídeo local o la cámara en tiempo real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create a method that reads a video file (using VideoCapture from OpenCV) and returns its frames along with video properties\n",
    "def read_video(videopath):\n",
    "    \"\"\"\n",
    "    Reads a video file and returns its frames along with video properties.\n",
    "\n",
    "    Args:\n",
    "        videopath (str): The path to the video file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - frames (list): A list of frames read from the video.\n",
    "            - frame_width (int): The width of the video frames.\n",
    "            - frame_height (int): The height of the video frames.\n",
    "            - frame_rate (float): The frame rate of the video.\n",
    "    \"\"\"\n",
    "\n",
    "    #TODO: Complete this line to read the video file\n",
    "    cap = cv2.VideoCapture(None) \n",
    "    \n",
    "    #TODO: Check if the video was successfully opened\n",
    "    if not cap.isOpened():\n",
    "        print('Error: Could not open the video file')\n",
    "\n",
    "    #TODO: Get the szie of frames and the frame rate of the video\n",
    "    frame_width = int(cap.get(None)) # Get the width of the video frames\n",
    "    frame_height = int(cap.get(None)) # Get the height of the video frames\n",
    "    frame_rate = cap.get(None) # Get the frame rate of the video\n",
    "    \n",
    "    #TODO: Use a loop to read the frames of the video and store them in a list\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = None\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return frames, frame_width, frame_height, frame_rate\n",
    "\n",
    "#TODO: Path to the video file (visiontraffic.avi)\n",
    "videopath = None  \n",
    "\n",
    "frames, frame_width, frame_height, frame_rate = read_video(videopath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tarea A.2**: Sustracción de Fondo mediante diferencia de frames\n",
    "Realice una sustracción de fondo mediante diferencia de frames, para ello guarde\n",
    "un frame con el fondo estático y úselo como frame de referencia de fondo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:  Show the frames to select the reference frame, press 'n' to move to the next frame and 's' to select the frame\n",
    "for i, frame in enumerate(frames):\n",
    "    #TODO: Show the frame\n",
    "    cv2.imshow('Video', None)\n",
    "    # Wait for the key\n",
    "    key = cv2.waitKey(0)\n",
    "    # If the key is 'n' continue to the next frame\n",
    "    if key == ord('n'):\n",
    "        continue\n",
    "    # If the key is 's' select the frame as the reference frame\n",
    "    elif key == ord('s'):\n",
    "        #TODO: Copy the frame to use it as a reference\n",
    "        reference_frame = None\n",
    "        #TODO: Convert the reference frame to grayscale\n",
    "        reference_frame = cv2.cvtColor(None, None)\n",
    "        print('Frame {} selected as reference frame'.format(i))\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "#TODO: Compute the difference between the reference frame and the rest of the frames and show the difference\n",
    "for frame in frames:\n",
    "    # Convert the frame to grayscale\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    #TODO: Compute the difference between the reference frame and the current frame\n",
    "    diff = cv2.absdiff(None, None)\n",
    "    cv2.imshow('Diferencia', diff)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tarea A.3**: Configuración de la sustracción de fondo con GMM\n",
    "Configure el sustractor de fondo usando el modelo de mezcla de gaussianas\n",
    "adaptativas (MOG2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Use MOG2 to detect the moving objects in the video\n",
    "\n",
    "history = None  # Number of frames to use to build the background model\n",
    "varThreshold = None  # Threshold to detect the background\n",
    "detectShadows = None  # If True the algorithm detects the shadows\n",
    "\n",
    "#TODO: Create the MOG2 object\n",
    "mog2 = cv2.createBackgroundSubtractorMOG2()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tarea A.4**: Aplicación de la Sustracción de Fondo\n",
    "\n",
    "Aplique la sustracción de fondo en cada frame para extraer los objetos en movimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Use a loop to detect the moving objects in the video using the MOG2 algorithm and \n",
    "# save a video storing the parameters at the name of the file\n",
    "\n",
    "#TODO: Create a folder to store the videos\n",
    "output_folder = None\n",
    "folder_path = os.path.join()\n",
    "if not os.path.exists():\n",
    "    os.makedirs()\n",
    "\n",
    "#TODO: Name of the output video file with the parameters (history, varThreshold, detectShadows)\n",
    "videoname = f'output_{None}_{None}_{None}.avi' # Name of the output video file with the parameters\n",
    "\n",
    "\n",
    "#TODO: Create a VideoWriter object to save the video\n",
    "fourcc = cv2.VideoWriter_fourcc(None) # Codec to use\n",
    "frame_size = (None, None) # Size of the frames\n",
    "fps = None # Frame rate of the video\n",
    "out = cv2.VideoWriter(None, None, None, None)\n",
    "\n",
    "for frame in frames:\n",
    "    #TODO: Apply the MOG2 algorithm to detect the moving objects\n",
    "    mask = mog2.apply(None)\n",
    "    #TODO: Convert to BGR the mask to store it in the video\n",
    "    mask = cv2.cvtColor(None, None)\n",
    "    #TODO: Save the mask in a video\n",
    "    out.write(None)\n",
    "\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preguntas del Apartado A**\n",
    "1. ¿Cómo afecta la variable `varThreshold` a la precisión de la detección?\n",
    "2. ¿Qué ventajas presenta `createBackgroundSubtractorMOG2` frente a métodos simples de diferencia de imágenes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Apartado B: Flujo Óptico**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tarea B.1**: Configuración del Flujo Óptico\n",
    "\n",
    "Consulte la documentación de `cv2.calcOpticalFlowPyrLK` para ver qué parametros se deben definir para realizar el cálculo del flujo óptico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Use the method to read the video file (slow_traffic_small.avi)\n",
    "videopath = None  # Path to the video file\n",
    "\n",
    "#TODO: Define the parameters for Lucas-Kanade optical flow\n",
    "winSize=(None, None)\n",
    "maxLevel=None\n",
    "criteria= (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tarea B.2**: Detección de Puntos de Interés\n",
    "\n",
    "Detecte los puntos de interés iniciales usando el algoritmo de Shi-Tomasi (`cv2.goodFeaturesToTrack`) en el primer frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Detect the initial points of interest in the first frame\n",
    "\n",
    "#TODO: Convert the first frame to grayscale\n",
    "prev_gray = cv2.cvtColor(None, None)\n",
    "\n",
    "#TODO: Define the parameters of the Shi-Tomasi algorithm\n",
    "mask = None\n",
    "maxCorners = None\n",
    "qualityLevel = None\n",
    "minDistance = None\n",
    "blockSize = None\n",
    "\n",
    "# Use the function goodFeaturesToTrack to detect the points of interest\n",
    "p0 = cv2.goodFeaturesToTrack(prev_gray, mask=mask, maxCorners=maxCorners, qualityLevel=qualityLevel, minDistance=minDistance, blockSize=blockSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tarea B.3**: Cálculo y Visualización del Flujo Óptico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Use a loop to track the points of interest in the rest of the frames\n",
    "\n",
    "# Create a mask image for drawing purposes\n",
    "mask = np.zeros_like(frame)\n",
    "\n",
    "for i, frame in enumerate(frames[1:]):\n",
    "    #TODO: Copy the frame\n",
    "    input_frame = None\n",
    "    # Convert the frame to grayscale\n",
    "    frame_gray = cv2.cvtColor(input_frame, cv2.COLOR_BGR2GRAY)\n",
    "    #TODO: Calculate the optical flow using the Lucas-Kanade algorithm\n",
    "    p1, st, err = cv2.calcOpticalFlowPyrLK()\n",
    "\n",
    "    # Select the points that were successfully tracked\n",
    "    good_new = p1[st == 1]\n",
    "    good_old = p0[st == 1]\n",
    "\n",
    "    # Draw the tracks\n",
    "    for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "        a, b = new.ravel().astype(int)\n",
    "        c, d = old.ravel().astype(int)\n",
    "        input_frame = cv2.circle(input_frame, (a, b), 5, (0, 0, 255), -1)\n",
    "        mask = cv2.line(mask, (a, b), (c, d), (0, 255, 0), 2)\n",
    "\n",
    "    #TODO: Update the inputs for the next iteration\n",
    "    prev_gray = None # Copy the current frame to the previous frame\n",
    "    p0 = None.reshape(-1, 1, 2) # Update the points to track\n",
    "    \n",
    "    # Show the frame with the tracks\n",
    "    cv2.imshow('Frame', cv2.add(input_frame, mask))\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preguntas del Apartado B**\n",
    "1. ¿Qué efecto tiene el parámetro `winSize` en la precisión del flujo óptico?\n",
    "2. ¿Cómo influye el parámetro `qualityLevel` en la función `cv2.goodFeaturesToTrack` al detectar puntos de interés?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Apartado C: Filtro de Kalman para Seguimiento de Objetos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tarea C.1**: Configuración del Filtro de Kalman\n",
    "\n",
    "Inicialice el filtro de Kalman (`cv2.KalmanFilter`) con una matriz de medición y transición adecuada para un seguimiento en dos dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Use the method to read the video file (slow_traffic_small.avi)\n",
    "videopath = None  # Path to the video file\n",
    "\n",
    "#TODO: Create the Kalman filter object\n",
    "kf = cv2.KalmanFilter(None, None)\n",
    "#TODO: Initialize the state of the Kalman filter\n",
    "kf.measurementMatrix =  None # Measurement matrix np.array of shape (2, 4) and type np.float32\n",
    "kf.transitionMatrix = None # Transition matrix np.array of shape (4, 4) and type np.float32\n",
    "kf.processNoiseCov = None # Process noise covariance np.array of shape (4, 4) and type np.float32\n",
    "\n",
    "measurement = np.array((2, 1), np.float32)\n",
    "prediction = np.zeros((2, 1), np.float32)\n",
    "\n",
    "#TODO: Show the frames to select the initial position of the object\n",
    "\n",
    "for i, frame in enumerate(frames):\n",
    "    # Show the frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "    # Wait for the key\n",
    "    key = cv2.waitKey(0)\n",
    "    # If the key is 'n' continue to the next frame\n",
    "    if key == ord('n'):\n",
    "        continue\n",
    "    # If the key is 's' select the position of the object\n",
    "    elif key == ord('s'):\n",
    "        # Select the position of the object\n",
    "        x, y, w, h = cv2.selectROI('Frame', frame, False)\n",
    "        track_window = (x, y, w, h)\n",
    "        #TODO: Compute the center of the object\n",
    "        cx = None\n",
    "        cy = None\n",
    "        #TODO: Initialize the state of the Kalman filter\n",
    "        kf.statePost = np.array([[None], [None], [0], [0]], np.float32)\n",
    "\n",
    "        # Initialize the covariance matrix\n",
    "        kf.errorCovPost = np.eye(4, dtype=np.float32)\n",
    "        \n",
    "        #Predict the position of the object\n",
    "        prediction = kf.predict()\n",
    "        \n",
    "        #TODO: Update the measurement and correct the Kalman filter\n",
    "        measurement = np.array([[None], [None]], np.float32)\n",
    "        kf.correct(measurement)\n",
    "\n",
    "        #TODO: Crop the object\n",
    "        crop = frame[None:None, None:None].copy()\n",
    "        #TODO: Convert the cropped object to HSV\n",
    "        hsv_crop = cv2.cvtColor()\n",
    "        #TODO: Compute the histogram of the cropped object (Reminder: Use only the Hue channel (0-180))\n",
    "        crop_hist = cv2.calcHist([None], [0], mask=None, histSize=[None], ranges=[None, None])\n",
    "        cv2.normalize(crop_hist, crop_hist, 0, 255, cv2.NORM_MINMAX)\n",
    "        \n",
    "        print(f'Initial position selected: {x}, {y}')\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tarea C.2**: Predicción y Corrección del Estado\n",
    "\n",
    "Realice la predicción del estado y corrija la posición estimada en cada iteración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Use the Kalman filter to predict the position of the points of interest\n",
    "\n",
    "term_crit = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 30, 1)\n",
    "\n",
    "for frame in frames[i:]:\n",
    "    #TODO: Copy the frame \n",
    "    input_frame = None\n",
    "    #TODO: Convert the frame to HSV\n",
    "    img_hsv = cv2.cvtColor()\n",
    "    \n",
    "    # Compute the back projection of the histogram\n",
    "    img_bproject = cv2.calcBackProject([img_hsv], [0], crop_hist, [0, 180], 1)\n",
    "    \n",
    "    # Apply the mean shift algorithm to the back projection\n",
    "    ret, track_window = cv2.meanShift(img_bproject, track_window, term_crit)\n",
    "    x_,y_,w_,h_ = track_window\n",
    "    #TODO: Compute the center of the object\n",
    "    c_x = None\n",
    "    c_y = None\n",
    "    \n",
    "    # Predict the position of the object\n",
    "    prediction = kf.predict()\n",
    "\n",
    "    #TODO: Update the measurement and correct the Kalman filter\n",
    "    measurement = np.array([[None], [None]], np.float32)\n",
    "    kf.correct(measurement)\n",
    "\n",
    "    \n",
    "    # Draw the predicted position\n",
    "    cv2.circle(input_frame, (int(prediction[0][0]), int(prediction[1][0])), 5, (0, 0, 255), -1)\n",
    "    cv2.circle(input_frame, (int(c_x), int(c_y)), 5, (0, 255, 0), -1)\n",
    "    \n",
    "    # Show the frame with the predicted position\n",
    "    cv2.imshow('Frame', input_frame)\n",
    "    key = cv2.waitKey(0)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preguntas del Apartado C**\n",
    "1. ¿Cómo afecta el valor de `transitionMatrix` a la predicción en el filtro de Kalman?\n",
    "2. ¿Cuál es la diferencia entre `measurementMatrix` y `transitionMatrix` en el contexto del seguimiento de objetos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ejercicio Adicional: Exploración del Modelo de Mezcla de Gaussianas (GMM)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objetivo**: Investigue cómo funciona el modelo de mezcla de gaussianas (GMM) para mejorar la detección en condiciones de iluminación cambiantes.\n",
    "\n",
    "1. Implementación del GMM: Utilice `cv2.createBackgroundSubtractorMOG()` y ajuste el parámetro `history` para observar cómo cambia la detección en función de la duración de la memoria del fondo.\n",
    "2. Comparación con `MOG2`: Observe las diferencias en la sensibilidad a las sombras y los cambios de iluminación. Pruebe con vídeos que incluyan cambios graduales de iluminación y objetos que se detienen temporalmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preguntas del Ejercicio Adicional**\n",
    "1. ¿Qué ventajas observa en `createBackgroundSubtractorMOG` en comparación con `createBackgroundSubtractorMOG2`?\n",
    "2. ¿Cómo afecta el parámetro `history` al rendimiento de detección en escenas con objetos que aparecen y desaparecen?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
